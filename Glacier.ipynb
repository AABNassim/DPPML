{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AABNassim/DPPML/blob/master/Glacier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd964ef",
      "metadata": {
        "id": "9cd964ef"
      },
      "source": [
        "# A simple hands-on exercise of glacier surface mass balance (SMB) modeling/reconstruction/forcasting using ML models in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd697b64",
      "metadata": {
        "id": "cd697b64"
      },
      "source": [
        "## Acknowledgement\n",
        "\n",
        "* ML in Glaciology Workshop (https://github.com/Machine-Learning-in-Glaciology-Workshop)\n",
        "* Dataset from WGMS, ERA5-Land and other authors\n",
        "* Jessy, Konrad, Tabea, Codrut"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa9cc6c",
      "metadata": {
        "id": "0fa9cc6c"
      },
      "source": [
        "## This hands-on tutorial consist of four main steps\n",
        "\n",
        "* Preprocessing\n",
        "* Data exploration\n",
        "* Training and testing\n",
        "* SMB reconstruction or forcast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RAqan65q5Q8l",
      "metadata": {
        "id": "RAqan65q5Q8l"
      },
      "outputs": [],
      "source": [
        "!pip install -qq geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2f2a56",
      "metadata": {
        "id": "5e2f2a56"
      },
      "outputs": [],
      "source": [
        "# Lets start with importing the required libraries\n",
        "# Data wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "\n",
        "# Geospatial packages\n",
        "import geopandas as gpd\n",
        "import xarray as xr\n",
        "\n",
        "# AI/ML packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import svm\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics\n",
        "from sklearn.inspection import permutation_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba49c02",
      "metadata": {
        "id": "8ba49c02"
      },
      "source": [
        "# 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25f1396",
      "metadata": {
        "id": "b25f1396"
      },
      "source": [
        "## 1.1. Get SMB data\n",
        "\n",
        "* Glaciological SMB data from the World Glacier Monitoring Service (WGMS), Zurich\n",
        "* Fluctuations of Glaciers (FoG) Database (https://wgms.ch/data_databaseversions/)\n",
        "* Other data from: Garg et al., 2021, STOTEN (https://doi.org/10.1016/j.scitotenv.2021.149533)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50fac9ae",
      "metadata": {
        "id": "50fac9ae"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "df = pd.read_csv('https://transfer.sh/4rd58J/WH_MB_1980_2020_GLAC_ELA.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8234579e",
      "metadata": {
        "id": "8234579e"
      },
      "source": [
        "* Temporal coverage of the SMB data is 41 years (1980-2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec6a106",
      "metadata": {
        "id": "0ec6a106"
      },
      "source": [
        "## 1.2. SMB data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8e9cc2",
      "metadata": {
        "id": "0f8e9cc2"
      },
      "outputs": [],
      "source": [
        "# Remove first two rows which are not (smb values) needed\n",
        "df_smb = df.iloc[2:]\n",
        "print('Total no. of SMB data points =', df_smb.count().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206a2caf",
      "metadata": {
        "id": "206a2caf"
      },
      "source": [
        "* Also, we need to reshape the dataframe from wide format (shown above) to long format using `pd.melt()`\n",
        "* We reshape the dataframe by `Year` column as identifier variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e841c6bf",
      "metadata": {
        "id": "e841c6bf"
      },
      "outputs": [],
      "source": [
        "df_smb = pd.melt(df_smb, id_vars =['Year'], value_vars = df_smb.columns)\n",
        "\n",
        "# Names of ‘variable’ and ‘value’ columns can be customized\n",
        "df_smb = df_smb.rename(columns={'variable': 'RGIId', 'value': 'smb'})\n",
        "df_smb[\"smb\"] = pd.to_numeric(df_smb[\"smb\"]) # convert \"smb\" column to numeric\n",
        "df_smb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353c45a7",
      "metadata": {
        "id": "353c45a7"
      },
      "source": [
        "## 1.3. Get glacier topographical data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55cb32e3",
      "metadata": {
        "id": "55cb32e3"
      },
      "source": [
        "* We use the glacier `RGIId` (Randolph Glacier Inventory) or `GLIMSId` (Global Land Ice Measurements from Space initiative) to extract these data from the inventory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b4b382",
      "metadata": {
        "id": "01b4b382"
      },
      "outputs": [],
      "source": [
        "# We transpose the column names (RGIId) as row/index to find the particular observation from RGI 6.0\n",
        "df_topo = df.transpose() \n",
        "df_topo = df_topo.iloc[1:,:2]\n",
        "df_topo.columns = ['Glacier_name', 'SMB_method']\n",
        "df_topo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42b9bec",
      "metadata": {
        "id": "a42b9bec"
      },
      "outputs": [],
      "source": [
        "# Open RGI inventory for 14. South Asia (West) shapefiles to extract info. for the selected glaciers\n",
        "gdf = gpd.read_file('https://transfer.sh/cgChks/rgi6_SAW_polygons.gpkg')\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "gdf.set_index(\"RGIId\", inplace = True)\n",
        "gdf['RGIId'] = gdf.index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81989c10",
      "metadata": {
        "id": "81989c10"
      },
      "source": [
        "#### RGI Regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a9f676",
      "metadata": {
        "id": "80a9f676"
      },
      "outputs": [],
      "source": [
        "Image(\"https://transfer.sh/irLRAF/RGI_Tech_Report_V6.0.pdf.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd72936",
      "metadata": {
        "id": "7bd72936"
      },
      "outputs": [],
      "source": [
        "# Get RGI columns/features of our interest\n",
        "df_topo_rgi = df_topo.merge(df_topo.merge(gdf[['RGIId', 'GLIMSId','geometry','Area','CenLon','CenLat','Zmin', 'Slope']], \n",
        "                                left_index=True, right_index=True, how='left', sort=False))\n",
        "df_topo_rgi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03fd8df3",
      "metadata": {
        "id": "03fd8df3"
      },
      "outputs": [],
      "source": [
        "# Replicating Zmin, Area, Slope columns for input (these features are constant every year)\n",
        "df_topo_rgi_repli = df_topo_rgi[['RGIId', 'Zmin', 'Area', 'Slope']]\n",
        "df_topo_rgi_repli = pd.DataFrame(np.repeat(df_topo_rgi_repli.values, 41, axis=0))\n",
        "df_topo_rgi_repli.columns = ['RGIId', 'Zmin', 'Area', 'Slope']\n",
        "df_topo_rgi_repli"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c8983a",
      "metadata": {
        "id": "05c8983a"
      },
      "source": [
        "* This will be required later while merging all variables/features together for training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04679fc4",
      "metadata": {
        "id": "04679fc4"
      },
      "source": [
        "* At this moment, we can also quickly visualise where are these glaciers geographically located in Asia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d38fed5d",
      "metadata": {
        "id": "d38fed5d"
      },
      "outputs": [],
      "source": [
        "# First, we need to convert the geometry (from RGI) to geodataframe using geopandas\n",
        "df_topo_rgi = gpd.GeoDataFrame(df_topo_rgi, crs=\"EPSG:4326\", geometry=df_topo_rgi.geometry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a72e3e2",
      "metadata": {
        "id": "2a72e3e2"
      },
      "outputs": [],
      "source": [
        "# Visualise\n",
        "fig, ax = plt.subplots()\n",
        "df_topo_rgi.plot(ax=ax)\n",
        "plt.title('Selected glaciers in the Western Himalaya (from RGI)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e3c4942",
      "metadata": {
        "id": "9e3c4942"
      },
      "source": [
        "## 1.4. Get glacier climate data\n",
        "\n",
        "* We use ERA5-Land reanalysis data at 9-km grid from the Climate Data Store (CDS, https://cds.climate.copernicus.eu/#!/home) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2910bde7",
      "metadata": {
        "id": "2910bde7"
      },
      "outputs": [],
      "source": [
        "# Processed climate data file for the WH region\n",
        "!wget -q https://transfer.sh/nauN37/era5l_1980_2020_all_variables_WH_TUM_winterschool2023.nc\n",
        "climate_ds = xr.open_dataset('era5l_1980_2020_all_variables_WH_TUM_winterschool2023.nc')\n",
        "climate_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b615e153",
      "metadata": {
        "id": "b615e153"
      },
      "source": [
        "#### Full name of the variables below:\n",
        "\n",
        "* cpdd = cumulative positive degree days\n",
        "* asn = forecast albedo\n",
        "* es = snow evaporation\n",
        "* sf = snowfall\n",
        "* skt = surface skin temperature\n",
        "* slhf = surface latent heat flux\n",
        "* sshf = surface sensible heat flux\n",
        "* smlt = snowmelt\n",
        "* sp = surface pressure\n",
        "* ssr = net solar radiation\n",
        "* ssrd = solar radiation downward\n",
        "* str = net thermal radiation\n",
        "* strd = thermal radiation downward\n",
        "* t2m = 2m temperature\n",
        "* tp = total precipitation\n",
        "* ws = wind speed\n",
        "\n",
        "#### Seasons\n",
        "\n",
        "* annual = 1 Oct to 30 Sept of next year\n",
        "* summer = Jun-Aug (JJAS)\n",
        "* Winter = Dec-Apr (DJFMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bfd3940",
      "metadata": {
        "id": "6bfd3940"
      },
      "source": [
        "* A number of meteorological/climate variables/features can be generated, here are several examples,\n",
        "* We generated/aggregated ~50 variables,\n",
        "* The variable will act as `predictors` of the SMB for a year for a particular glacier, \n",
        "* Notice here is the `hydrological year (HY)`, to keep consistency with SMB year \n",
        "* But one might be curious how the climate varies spatially over the ROI,\n",
        "* Lets quickly visualise the climate (`t2m`) of the ROI with glaciers over it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7181f967",
      "metadata": {
        "id": "7181f967"
      },
      "outputs": [],
      "source": [
        "# Visualise\n",
        "fig, ax = plt.subplots()\n",
        "climate_ds.t2m_annual[0].plot(ax=ax)\n",
        "df_topo_rgi.plot(ax=ax, color='k')\n",
        "plt.title('Glaciers (RGI) and Climate in 1980')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feeace40",
      "metadata": {
        "id": "feeace40"
      },
      "source": [
        "### Now, extract the climate data for each glacier for every HY using centroid values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34fd7398",
      "metadata": {
        "id": "34fd7398"
      },
      "outputs": [],
      "source": [
        "# We create required objects\n",
        "lat = df_topo_rgi.CenLat\n",
        "lon = df_topo_rgi.CenLon\n",
        "name = df_topo_rgi.RGIId\n",
        "\n",
        "# Start extracting the process for all the glacier centroids\n",
        "# Code help: https://stackoverflow.com/questions/58635776/python-extract-multiple-lat-long-from-netcdf-files-using-xarray\n",
        "Newdf = pd.DataFrame([])\n",
        "\n",
        "for i,j,id in zip(lat,lon,name):\n",
        "    dsloc = climate_ds.sel(latitude=i,longitude=j,method='nearest')\n",
        "    DT=dsloc.to_dataframe()\n",
        "\n",
        "    # insert the name with your preferred column title:\n",
        "    DT.insert(loc=0,column=\"RGIId\",value=id)\n",
        "    Newdf=Newdf.append(DT,sort=True)\n",
        "\n",
        "print(Newdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154a4657",
      "metadata": {
        "id": "154a4657"
      },
      "outputs": [],
      "source": [
        "# A bit more preprocessing before the data can be used for training\n",
        "Newdf['HY'] = Newdf.index\n",
        "Newdf = Newdf.reset_index(drop=True)\n",
        "Newdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438b39b8",
      "metadata": {
        "id": "438b39b8"
      },
      "source": [
        "### At this moment, we have all variables (e.g., SMB, topographic data and climate data)\n",
        "* Lets merge them all to make final training file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5734b93",
      "metadata": {
        "id": "a5734b93"
      },
      "outputs": [],
      "source": [
        "# First, merge topographic features to the climate features dataframe\n",
        "Newdf['Zmin'] = df_topo_rgi_repli['Zmin']\n",
        "Newdf['Area'] = df_topo_rgi_repli['Area']\n",
        "Newdf['Slope'] = df_topo_rgi_repli['Slope']\n",
        "\n",
        "# Second, merge SMB values\n",
        "Newdf = pd.concat([Newdf, df_smb.smb], axis=1, ignore_index=False, sort=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "961499e6",
      "metadata": {
        "id": "961499e6"
      },
      "source": [
        "# 2. Data exploration "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3227520f",
      "metadata": {
        "id": "3227520f"
      },
      "source": [
        "* One can start with the basic statistics to see if there is any issue with the final data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808498d9",
      "metadata": {
        "id": "808498d9"
      },
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "Newdf.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f8544d",
      "metadata": {
        "id": "41f8544d"
      },
      "outputs": [],
      "source": [
        "# Then, SMB data frquency in each HY\n",
        "mb_hy = Newdf.groupby(Newdf['HY']).count()\n",
        "mb_hy = mb_hy['smb']\n",
        "mb_hy.plot.bar()\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('No. of SMB data points for training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2cc9a5",
      "metadata": {
        "id": "6a2cc9a5"
      },
      "source": [
        "* Final processing step before feeding the data for training/testing\n",
        "* Note that we have some SMB data gaps in different years, we need to remove those observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de074d17",
      "metadata": {
        "id": "de074d17"
      },
      "outputs": [],
      "source": [
        "# Final data\n",
        "data = Newdf[Newdf['smb'].notna()] # remove NaN rows\n",
        "\n",
        "# In addition, we set HY and RGIId as index so that all glaciers/years are grouped \n",
        "# This will prevent any spatiotemporal infomration leakage while splitting \n",
        "data = data.set_index(['RGIId', 'HY'])\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1324270",
      "metadata": {
        "id": "c1324270"
      },
      "source": [
        "# 3. Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1132ac",
      "metadata": {
        "id": "7e1132ac"
      },
      "outputs": [],
      "source": [
        "# Before feeding the data for training/testing, lets check the shape\n",
        "print('The shape of our features is:', data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2788c41c",
      "metadata": {
        "id": "2788c41c"
      },
      "outputs": [],
      "source": [
        "# Make the training and target sets\n",
        "X = data.drop('smb', axis=1)  # Features/training, here we selected all feature/variables except SMB which is our target\n",
        "y = data['smb']  # Labels/target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2086b8d",
      "metadata": {
        "id": "d2086b8d"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training set and test set\n",
        "# 70% training and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec14b55e",
      "metadata": {
        "id": "ec14b55e"
      },
      "source": [
        "#### Lets have a quick look at the shapes of the training/testing sets that we just built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88c57ef",
      "metadata": {
        "id": "b88c57ef"
      },
      "outputs": [],
      "source": [
        "print('Training Features Shape:', X_train.shape)\n",
        "print('Training Labels Shape:', y_train.shape)\n",
        "print('Testing Features Shape:', X_test.shape)\n",
        "print('Testing Labels Shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bce6a71",
      "metadata": {
        "id": "4bce6a71"
      },
      "source": [
        "* We are ready to train a ML model\n",
        "* For the sake of simplicity, we applied Random Forest Regressor model, which is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.\n",
        "* More details: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e144c1",
      "metadata": {
        "id": "c1e144c1"
      },
      "outputs": [],
      "source": [
        "# Call a RFR model\n",
        "rfr = RandomForestRegressor(n_estimators=100, bootstrap=True, random_state = 42) # Default parameters\n",
        "\n",
        "# We can look at the parameters used by our current model\n",
        "rfr.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f78440a",
      "metadata": {
        "id": "4f78440a"
      },
      "source": [
        "* For the sake of simplicity and limited time, we do not change anything in the model parameters,\n",
        "* Lets fit our training data to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e886911",
      "metadata": {
        "id": "0e886911"
      },
      "outputs": [],
      "source": [
        "# Fit the model on training data\n",
        "rfr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff3e07c",
      "metadata": {
        "id": "cff3e07c"
      },
      "outputs": [],
      "source": [
        "# Lets check the model performance while training and testing (R2)\n",
        "print(f\"model train set performance: {rfr.score(X_train, y_train):.4f}\")\n",
        "print(f\"model test set performance: {rfr.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783f0391",
      "metadata": {
        "id": "783f0391"
      },
      "source": [
        "#### At the current configuration, the model is overfitting as both training/testing `r2` is far from each other, which should be closer.\n",
        "\n",
        "* We can have a look at other performance scores, i.e., RMSE, MSE, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e88b96",
      "metadata": {
        "id": "18e88b96"
      },
      "outputs": [],
      "source": [
        "# Check the MSE and RMSE score on training set\n",
        "# predict the test data\n",
        "y_pred = rfr.predict(X_train)\n",
        "print('TRAINING')\n",
        "print('R^2:', metrics.r2_score(y_train, y_pred))\n",
        "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_train, y_pred))\n",
        "print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(y_train, y_pred, squared=False))\n",
        "\n",
        "# Check the MSE and RMSE score on testing set\n",
        "y_pred = rfr.predict(X_test)\n",
        "print('TESTING')\n",
        "print('R^2:', metrics.r2_score(y_test, y_pred))\n",
        "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(y_test, y_pred, squared=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f796cf",
      "metadata": {
        "id": "64f796cf"
      },
      "source": [
        "#### We can also have a look at the original/predicted SMB data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345cf040",
      "metadata": {
        "id": "345cf040"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "y_pred = rfr.predict(X_test)\n",
        "\n",
        "# visualise the prediction using a 1:1 scatter plot\n",
        "plt.figure()\n",
        "plt.scatter(y_pred, y_test, alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='k')\n",
        "plt.xlabel('predicted SMB (m w.e.)')\n",
        "plt.ylabel('original SMB (m w.e.)')\n",
        "\n",
        "plt.annotate(\"r-squared = {:.2f}\".format(metrics.r2_score(y_test, y_pred)), (-1.9, 0.25))\n",
        "plt.annotate(\"mse = {:.2f}\".format(metrics.mean_squared_error(y_test, y_pred))  + str(' m w.e.'), (-1.9, 0.15))\n",
        "plt.annotate(\"rmse = {:.2f}\".format(metrics.mean_squared_error(y_test, y_pred, squared=False)) + str(' m w.e.'), (-1.9, 0.05))\n",
        "\n",
        "plt.title(label=str('RandomForestRegressor()'))\n",
        "plt.grid(alpha=0.4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8158dc4",
      "metadata": {
        "id": "b8158dc4"
      },
      "source": [
        "## For the sake of simplicity and time, we did not show any `cross_validation` appraoch to improve the model or its performance, which is often used for better validation and predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1800075b",
      "metadata": {
        "id": "1800075b"
      },
      "source": [
        "## Feature/predictor importances\n",
        "\n",
        "* To know what are the meteorological/topographical drivers of SMB\n",
        "* We make use of sklearn `permutation_importance()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbffce63",
      "metadata": {
        "id": "bbffce63"
      },
      "outputs": [],
      "source": [
        "result = permutation_importance(rfr, X_test, y_test, n_repeats=10, random_state=0, n_jobs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "103d47d5",
      "metadata": {
        "id": "103d47d5"
      },
      "outputs": [],
      "source": [
        "# Get the column/feature names\n",
        "df_annual_train_columns = data.drop('smb', axis=1)\n",
        "\n",
        "# Plot the importances\n",
        "fig, ax = plt.subplots(figsize=(6,7))\n",
        "sorted_idx = result.importances_mean.argsort()\n",
        "ax.boxplot(\n",
        "    result.importances[sorted_idx].T*100, vert=False, labels=df_annual_train_columns.columns[sorted_idx]\n",
        ")\n",
        "ax.set_title(\"Permutation importance of each feature/variable\")\n",
        "ax.set_ylabel(\"features\")\n",
        "ax.set_xlabel(\"importance [%]\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830c98e5",
      "metadata": {
        "id": "830c98e5"
      },
      "source": [
        "### Now, our model is ready to forcast or predic SMB for glaciers where we do not have any observation.\n",
        "\n",
        "* We can get features from ERA5-Land or RGI or any other reanalysis/gridded open datasets, i.e., HAR, CMIP6, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6c0684",
      "metadata": {
        "id": "8e6c0684"
      },
      "source": [
        "# 4. Glacier SMB Reconstruction/Forcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8368c724",
      "metadata": {
        "id": "8368c724"
      },
      "source": [
        "* First, we use this model where we have some observe data and see how the model works there,\n",
        "* Chhota Shigri Glacier - a benchmark glacier in the HKH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b45b3c",
      "metadata": {
        "id": "14b45b3c"
      },
      "outputs": [],
      "source": [
        "temp_df = data.reset_index()\n",
        "cs_smb  = temp_df[temp_df.RGIId == 'RGI60-14.15990'] # We select the Chhota Shigri Glacier, where observations are available\n",
        "cs_smb = cs_smb.set_index(['RGIId', 'HY'])\n",
        "cs_smb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78842756",
      "metadata": {
        "id": "78842756"
      },
      "outputs": [],
      "source": [
        "# Predict SMB for Chhota Shigri Glacier\n",
        "cs_smb_pred = cs_smb.iloc[:,:-1] # We dropped the observed SMB column to feed it into the model\n",
        "smb_pred = rfr.predict(cs_smb_pred)\n",
        "\n",
        "# Merge the SMB values with the feature/RGIId dataframe\n",
        "cs_smb['smb_pred'] = smb_pred\n",
        "cs_smb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e409da44",
      "metadata": {
        "id": "e409da44"
      },
      "outputs": [],
      "source": [
        "# Plot and see\n",
        "cs_prediction = cs_smb.reset_index()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(cs_prediction.HY, cs_prediction.smb, label = 'observation', marker='o')\n",
        "plt.plot(cs_prediction.HY, cs_prediction.smb_pred, label = 'ML model', marker='o')\n",
        "plt.axhline(0, linestyle='--', alpha=0.4)\n",
        "\n",
        "plt.annotate(\"Mean obs. = {:.2f}\".format(cs_prediction.smb.mean()) + str(' m w.e.'), (2003, 0.7))\n",
        "plt.annotate(\"Mean ML mod. = {:.2f}\".format(cs_prediction.smb_pred.mean()) + str(' m w.e.'), (2003, 0.6))\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Annual SMB (m w.e.)')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "868dc308",
      "metadata": {
        "id": "868dc308"
      },
      "source": [
        "* Second, lets select 10 random glaciers from the Western Himalaya to reconstruct their mass balance with the model that we trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4e34f8",
      "metadata": {
        "id": "ab4e34f8"
      },
      "outputs": [],
      "source": [
        "# Prediction data, preprocessed for random 10 glaciers\n",
        "df_prediction = pd.read_csv('https://transfer.sh/DL2Dxn/TUMwinterschool_ML_SMB_Glacier_predictions.csv')\n",
        "df_prediction = df_prediction.set_index(['RGIId', 'HY'])\n",
        "df_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ef8044",
      "metadata": {
        "id": "b2ef8044"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "smb_pred = rfr.predict(df_prediction)\n",
        "\n",
        "# Merge the SMB values with the feature/RGIId dataframe\n",
        "df_prediction['smb_pred'] = smb_pred\n",
        "df_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d2f886d",
      "metadata": {
        "id": "4d2f886d"
      },
      "source": [
        "* We can plot them to look at their values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ecbcdf",
      "metadata": {
        "id": "49ecbcdf"
      },
      "outputs": [],
      "source": [
        "df_prediction_rgiid = df_prediction.reset_index()\n",
        "\n",
        "# Using seaborn we can quickly plot all the glacier SMB values together \n",
        "sns.lineplot(data=df_prediction_rgiid, x='HY', y='smb_pred', hue='RGIId', alpha=0.4, palette='viridis')\n",
        "plt.show()\n",
        "\n",
        "# Individual plots\n",
        "# df_prediction.reset_index().groupby('RGIId').plot('HY', 'smb_pred')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51450484",
      "metadata": {
        "id": "51450484"
      },
      "source": [
        "# Thank you! \n",
        "\n",
        "# We are happy to have further discussion on model improvement, other ideas, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "* Steiner, D., Walter, A., and Zumbühl, H.: The application of a non-linear back-propagation neural network to study the mass balance of Grosse Aletschgletscher, Switzerland, J. Glaciol., 51, 313–323, https://doi.org/10.3189/172756505781829421, 2005.\n",
        "\n",
        "* Bolibar, J., Rabatel, A., Gouttevin, I., Galiez, C., Condom, T., and Sauquet, E.: Deep learning applied to glacier evolution modelling, The Cryosphere, 14, 565–584, https://doi.org/10.5194/tc-14-565-2020, 2020\n",
        "\n",
        "* Anilkumar, R., Bharti, R., Chutia, D., and Aggarwal, S. P.: Modelling the Point Mass Balance for the Glaciers of Central European Alps using Machine Learning Techniques, EGUsphere [preprint], https://doi.org/10.5194/egusphere-2022-1076, 2022."
      ],
      "metadata": {
        "id": "KC96_hmALOTK"
      },
      "id": "KC96_hmALOTK"
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}